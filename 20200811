如何设置F1-F12功能键不需要按Fn键就能用？--> 按下FN+ 键盘左上角的 ESC键 即可

表添加主键：alter table user add primary key (id);（主键索引）
唯一的索引 (Unique Index)： CREATE UNIQUE INDEX 索引名称 ON 表名称 (列名称) 
简单的索引：CREATE INDEX 索引名称 ON 表名称 (列名称)
为多列创建组合索引：CREATE INDEX PersonIndex ON Person (LastName, FirstName) ,遵循最左前缀原则
删除索引：alter table table_name drop index index_name ;

主键设置自增长：alter table tablename modify id int(11)  auto_increment;
设置主键自增长从1开始：alter table t_user_register_info auto_increment = 1; 


queryForMap()  query()  queryForList():查询结果为空，不会抛异常 ，返回null
queryForObject():查询的结果集为空 或者 > 1, 抛异常


git clone 服务的仓库地址  远程项目拷贝到本地
git branch 查看本地分支
git branch -a 查看所有本地及远程仓库分支
git checkout dev 切换本地分支
git reset --hard HEAD^ 本地往后回退一个版本  ，如果已同步到远程，远程回退 git push --force
merge合并是冲突 git reset --hard head
切换到远程的其他分支:
1.git checkout -b branchnew origin/branchnew
2.git branch branchnew origin/branchnew     git checkout branchnew

redis-server.exe redis.windows.conf
redis-cli.exe -h 127.0.0.1 -p 6379

idea设置新窗口打开一个项目：File --> Settings --> Appearance & Behavior --> System Settings --> 勾选Open project in new window
添加一个新项目 ：File --> Open
如果打开多个窗口，切换项目：File --> Close Project   (点击X号，可以从idea中移除项目，本地磁盘文件会保留)

redis的2种客户端：  jedis  lettuce
    区别：lettuce 基于Netty NIO 来管理Redis连接，多线程并发访问时，多个线程可以共享同一个RedisConnection
          jedis 推出的比较早，多线程并发访问时，只有使用连接池，为每个Jedis实例创建连接

nacos和eureka:注册中心

启动本地mongodb服务：
    找到mongodb的安装目录，在bin目录下执行以下命令行--> 1：mongod --config D:\mongodb\mongo.conf(不需要认证，没有反应 但其实启动成功)   mongod --auth --config D:\mongodb\mongo.conf(用户认证才可以操作)
                                                        2: 重新打开一个命令行，在bin 目录下执行 mongo(启动成功，show dbs 可以查看)      如果开启登陆认证，
                                                                                                                                         方案一：mongo -u "adminUser" -p "adminPass" --authenticationDatabase "admin"
                                                                                                                                         方案二：客户端连接后，再进行验证 第一步：use db   第二部：db.auth("username","password")
mongodb数据库创建用户：
        db.createUser({user:"用户名",pwd:"密码",roles:[{role:"readWrite",db:"当前数据库"}]})                                                                                                                                         

            
配置中心config server 利用github的webhook机制实现自动刷新config client配置：监测到代码push后，回调url: http://localhost:8866/actuator/bus-refresh           
                                                                                                                                                                                                                                                                      
微信换行：Shift + Enter                                          

使用spring jdbc 防sql注入：
    like的两种写法：1(?占位符). like ?   list.add("%" + ** + "%")      2(绑定参数)  like concat('%',:**,'%')       self.productName = paramReplace(self..productName);
    
mysql使用left join 左外连接， where右表条件无效，变成inner join

<dependency>
	<groupId>com.alibaba</groupId>
	<artifactId>fastjson</artifactId>
	<version>1.1.34</version>
</dependency>

<dependency>
     <groupId>org.projectlombok</groupId>
     <artifactId>lombok</artifactId>
     <optional>true</optional>
</dependency>

java代码添加日志打印：
添加依赖：
<dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
</dependency>

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

private final static Logger logger = LoggerFactory.getLogger(**.class);
logger.info("入参：{},返回结果：{}",accountId,shopCode);

JSONObject解析入参中的list:
    JSONArray accountIdList = json.getJSONArray("accountIdList");
    List<Long> idList = JSONObject.parseArray(c.getJSONArray("accountIdList").toJSONString(), Long.class);

git上传报错总结：
    1.提示出错信息：fatal: remote origin already exists.
        解决办法如下：
            1、先输入 git remote rm origin
            2、再输入 git remote add origin  https://github.com/(user_name)/(app_name).git 就不会报错了！
    2.Permission denied（publickey）
         解决办法如下：ssh-keygen -t rsa -C"sec***@163.com" 
    3.报错 failed to push some refs to ‘git@github.com:xxx/xxx.git‘(原因：本地仓库没有README.md文件)  
         解决办法如下：git pull --rebase origin master 把代码合并 ，再执行git push origin master 
D:/spring-boot目录对应github远程仓库spring-boot,把项目放到本地spring-boot目录下，git add . --> git commit -m '' --> git push 提交到远程

git checkout -b 分支名  --> 本地创建该分支并切换到该分支
git add .
git commit -m ''
git push --set-upstream origin 分支名（首次本地分支同步远程）  -->提交本地分支到远程，远程分支添加成功


spring-cloud-zuul网关：
        filter是zuul的核心，有4种类型， 支持自定义filter,需要继承ZuulFilter
               pre(请求被路由之前调用)
               routing(将请求路由到微服务)
               post(路由到微服务以后执行，将响应从微服务发送给客户端)
               error(报错时执行)

linux 查看日志指令：
    1. tail -f ***.log  查询实时日志，默认最后10行    tail -n 20 filename (显示filename最后20行)
        ctrl+c 退出tail命令
    2. 搜索关键字附近的日志
        cat -n filename |grep "关键字"
    3. 进入编辑查找:vi(vim)
        1、进入vim编辑模式:vim filename
        2、输入“/关键字”,按enter键查找
        3、查找下一个,按“n”即可
            退出:按ESC键后,接着再输入:号时,vi会在屏幕的最下方等待我们输入命令
            wq! 保存退出;
            q! 不保存退出;
     4.分页查看
        more ***.log  ctrl+f:向下滚动一页  回车键：向下移动一行    空格键：向下移动一页    q：退出more

kafka windows环境下启动： D:\kafka\kafka_2.12-2.3.0\bin\windows
    kafka server：kafka-server-start.bat ..\..\config\server.properties
    创建topic: kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testDemo
    查看topic: kafka-topics.bat --list --zookeeper localhost:2181
    查看某个topic分区副本详情：kafka-topics.bat --zookeeper localhost:2181 --describe --topic testDemo
    打开Producer: kafka-console-producer.bat --broker-list localhost:9092 --topic testDemo
    打开consumer: kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic testDemo (--from-beginning)可省略   zookeeper高版本需要这样写Cbootstrap-server

    kafka producer 发送消息分区选择策略：会根据分区规则选择把消息存储到哪个分区中，只要如果分区规则设置的合理，那么所有的消息将会被均匀的分布到不同的分区中,
                                       这样就实现了负载均衡和水平扩展
            1.指定分区，此时key不起作用    new ProducerRecord(topic, partition, key, data)
            2.不指定分区，又可以分两种情况，指定key,不指定Key，key的作用是为消息选择存储分区
                2.1 指定key  new ProducerRecord(topic, key, data);  根据key的hash值与分区数取模来决定数据存储到那个分区
                2.2 key=null, new ProducerRecord(topic, data);
                        Kafka并不是每条消息都随机选择一个Partition,先从缓存中取分区号，然后判断缓存的值是否为空，如果不为空，就将消息存到这个分区，
                    当缓存过时之后,重新计算要存储的分区，并将分区号缓存起来，供下次使用

redis实现分布式锁，由于setnx和expire（设置超时时间）不是原子操作，当setnx成功后，redis宕掉，则此锁无法释放掉，因此2.6.12版本后，使用set指令，通过参数来实现setnx功能。
    （key,value,nxxx,expx,time）: nx-不存在时设置  ex-秒  px-毫秒

需要用到Base64,引入依赖
public static byte[] decode(String base64) throws Exception {
    return Base64.decodeBase64(base64.getBytes());
}
public static String encode(byte[] bytes) throws Exception {
    return new String(Base64.encodeBase64(bytes));
}
<dependency>
      <groupId>commons-codec</groupId>
      <artifactId>commons-codec</artifactId>
</dependency>
